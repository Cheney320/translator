{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme</th>\n",
       "      <th>nl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 millón de afectados ante las inundaciones en...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 millón de fans en facebook antes del 14 de f...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 satellite galileo sottoposto ai test presso ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 der welt sind bei</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 jaar voor overval op juwelier bejaard echtp...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme  \\\n",
       "0  1 millón de afectados ante las inundaciones en...                                      \n",
       "1  1 millón de fans en facebook antes del 14 de f...                                      \n",
       "2  1 satellite galileo sottoposto ai test presso ...                                      \n",
       "3                               10 der welt sind bei                                      \n",
       "4  10 jaar voor overval op juwelier bejaard echtp...                                      \n",
       "\n",
       "   nl  \n",
       "0  es  \n",
       "1  es  \n",
       "2  it  \n",
       "3  de  \n",
       "4  nl  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/new_data.csv', encoding='utf8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看语种分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es    1562\n",
       "fr    1551\n",
       "it    1539\n",
       "en    1505\n",
       "ch    1500\n",
       "de    1479\n",
       "nl    1429\n",
       "Name: nl, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['nl'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = data['1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme'], data['nl']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词袋模型+n-gram模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(\n",
    "    lowercase=True,  # 英文文本全小写\n",
    "    analyzer='char_wb',  # 逐个字母解析\n",
    "    ngram_range=(1, 3),  # 1=出现的字母以及每个字母出现的次数，2=出现的连续2个字母，和连续2个字母出现的次数\n",
    "    # trump images are now... => 1gram=t,r,u,m,p... 2gram=tr,ru,um,mp...\n",
    "    max_features=1000,   # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 74 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = vec.transform([\"I love machine learning.I think this is a magic field\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' 1', ' 2', ' 20', ' 3', ' a', ' a ', ' ab', ' ac', ' af', ' al', ' am', ' an', ' ap', ' ar', ' as', ' at', ' au', ' av', ' b', ' ba', ' be', ' bi', ' bl', ' bo', ' br', ' bu', ' c', ' ca', ' ce', ' ch', ' ci', ' cl', ' co', ' cr', ' d', ' da', ' de', ' di', ' do', ' du', ' dé', ' e', ' e ', ' ee', ' eg', ' ei', ' el', ' en', ' er', ' es', ' et', ' f', ' fa', ' fe', ' fi', ' fo', ' fr', ' g', ' ga', ' ge', ' gi', ' go', ' gr', ' h', ' ha', ' he', ' hi', ' ho', ' i', ' i ', ' il', ' im', ' in', ' is', ' it', ' j', ' ja', ' je', ' jo', ' k', ' ka', ' ki', ' ko', ' l', ' la', ' le', ' li', ' lo', ' m', ' ma', ' me', ' mi', ' mo', ' mu', ' n', ' na', ' ne', ' ni', ' no', ' nu', ' o', ' of', ' on', ' op', ' ou', ' p', ' pa', ' pe', ' pi', ' pl', ' po', ' pr', ' pu', ' q', ' qu', ' r', ' ra', ' re', ' ri', ' ro', ' s', ' sa', ' sc', ' se', ' si', ' so', ' sp', ' st', ' su', ' t', ' ta', ' te', ' th', ' ti', ' to', ' tr', ' tu', ' tw', ' u', ' un', ' us', ' v', ' va', ' ve', ' vi', ' vo', ' w', ' wa', ' we', ' wi', ' wo', ' y', ' y ', ' yo', ' z', ' zi', ' zu', '0', '0 ', '00', '00 ', '01', '1', '1 ', '10', '2', '2 ', '20', '201', '3', '4', '5', '6', '7', '8', '9', 'a', 'a ', 'aa', 'aan', 'aar', 'ab', 'abe', 'ac', 'acc', 'ace', 'ach', 'aci', 'act', 'ad', 'ad ', 'ade', 'ado', 'af', 'ag', 'ag ', 'age', 'ah', 'ai', 'ain', 'air', 'ais', 'ait', 'ak', 'ak ', 'al', 'al ', 'ale', 'ali', 'all', 'am', 'am ', 'ame', 'amo', 'an', 'an ', 'ana', 'anc', 'and', 'ang', 'ani', 'ank', 'ann', 'ano', 'ans', 'ant', 'ap', 'app', 'ar', 'ar ', 'ara', 'ard', 'are', 'ari', 'ark', 'arr', 'art', 'as', 'as ', 'ass', 'ast', 'at', 'at ', 'ate', 'ati', 'ato', 'att', 'au', 'au ', 'av', 'ave', 'ay', 'ay ', 'az', 'b', 'b ', 'ba', 'bar', 'be', 'ben', 'ber', 'bi', 'bl', 'ble', 'blo', 'bo', 'br', 'bre', 'bu', 'c', 'c ', 'ca', 'ca ', 'can', 'car', 'cat', 'cc', 'ce', 'ce ', 'ces', 'ch', 'ch ', 'cha', 'che', 'chi', 'cho', 'cht', 'ci', 'ci ', 'cia', 'ck', 'ck ', 'cl', 'co', 'co ', 'col', 'com', 'con', 'cor', 'cou', 'cr', 'ct', 'cti', 'cu', 'd', 'd ', 'da', 'da ', 'dan', 'das', 'dat', 'day', 'de', 'de ', 'del', 'dem', 'den', 'der', 'des', 'di', 'di ', 'dia', 'die', 'dir', 'dis', 'do', 'do ', 'dr', 'ds', 'ds ', 'du', 'du ', 'dé', 'e', 'e ', 'ea', 'eb', 'ec', 'ect', 'ed', 'ed ', 'ede', 'edi', 'ee', 'een', 'eer', 'eet', 'ef', 'ef ', 'eg', 'egy', 'eh', 'ei', 'ei ', 'ein', 'ek', 'el', 'el ', 'ele', 'eli', 'ell', 'em', 'ema', 'eme', 'emo', 'emp', 'en', 'en ', 'ena', 'enc', 'end', 'ene', 'ens', 'ent', 'eo', 'ep', 'er', 'er ', 'era', 'erd', 'ere', 'eri', 'erl', 'ern', 'ero', 'err', 'ers', 'ert', 'erv', 'es', 'es ', 'esc', 'ese', 'esp', 'ess', 'est', 'et', 'et ', 'ete', 'ett', 'eu', 'eur', 'eut', 'ev', 'eve', 'ew', 'ex', 'ez', 'f', 'f ', 'fa', 'fe', 'ff', 'fi', 'fin', 'fo', 'for', 'fr', 'fra', 'ft', 'fu', 'g', 'g ', 'ga', 'ge', 'ge ', 'gen', 'ger', 'ges', 'gg', 'gh', 'gi', 'gl', 'gli', 'gn', 'go', 'gr', 'gra', 'gu', 'gy', 'gyp', 'h', 'h ', 'ha', 'han', 'has', 'hat', 'he', 'he ', 'hen', 'her', 'het', 'hi', 'ho', 'hon', 'hr', 'ht', 'ht ', 'hu', 'i', 'i ', 'ia', 'ia ', 'ial', 'ian', 'ib', 'ic', 'ica', 'ice', 'ich', 'ici', 'id', 'id ', 'ida', 'ide', 'ie', 'ie ', 'ien', 'ier', 'ies', 'if', 'ig', 'ij', 'ik', 'il', 'il ', 'ile', 'ili', 'ill', 'im', 'im ', 'ime', 'in', 'in ', 'ina', 'ind', 'ine', 'inf', 'ing', 'ini', 'ins', 'int', 'io', 'io ', 'ion', 'ip', 'ir', 'ir ', 'ire', 'is', 'is ', 'isc', 'isi', 'iss', 'ist', 'it', 'it ', 'ita', 'ite', 'iti', 'itt', 'iv', 'ive', 'iz', 'j', 'j ', 'ja', 'je', 'je ', 'jo', 'ju', 'k', 'k ', 'ka', 'ke', 'ke ', 'ken', 'ki', 'ko', 'koz', 'ks', 'ks ', 'kt', 'l', 'l ', 'la', 'la ', 'lan', 'las', 'lat', 'ld', 'ld ', 'le', 'le ', 'lea', 'len', 'les', 'lg', 'li', 'li ', 'lia', 'lic', 'lie', 'lin', 'lis', 'lit', 'liv', 'll', 'll ', 'lla', 'lle', 'lli', 'llo', 'lo', 'lo ', 'log', 'los', 'ls', 'ls ', 'lt', 'lu', 'ly', 'm', 'm ', 'ma', 'ma ', 'man', 'mar', 'mb', 'me', 'me ', 'men', 'mer', 'met', 'mi', 'mil', 'min', 'mis', 'mm', 'mme', 'mo', 'mo ', 'mon', 'mos', 'mp', 'ms', 'mu', 'n', 'n ', 'na', 'na ', 'nal', 'nat', 'nc', 'nce', 'nci', 'nd', 'nd ', 'nda', 'nde', 'ndi', 'ndo', 'ne', 'ne ', 'nen', 'ner', 'nes', 'nf', 'ng', 'ng ', 'nge', 'ni', 'ni ', 'nic', 'nie', 'nis', 'nk', 'nl', 'nn', 'nn ', 'nne', 'no', 'no ', 'non', 'nos', 'ns', 'ns ', 'nse', 'nst', 'nt', 'nt ', 'nta', 'nte', 'nti', 'nto', 'ntr', 'nu', 'nv', 'nz', 'o', 'o ', 'ob', 'oc', 'och', 'od', 'oe', 'of', 'of ', 'og', 'og ', 'oi', 'ok', 'ok ', 'ol', 'ola', 'ole', 'oli', 'oll', 'om', 'om ', 'ome', 'omm', 'on', 'on ', 'ona', 'ond', 'one', 'oni', 'ono', 'ons', 'ont', 'oo', 'ook', 'oor', 'op', 'op ', 'or', 'or ', 'ora', 'ord', 'ore', 'ori', 'orm', 'ort', 'os', 'os ', 'ost', 'ot', 'ote', 'ou', 'ou ', 'our', 'ous', 'out', 'ouv', 'ov', 'ove', 'ow', 'ow ', 'oy', 'oz', 'ozy', 'p', 'p ', 'pa', 'par', 'pas', 'pe', 'pen', 'per', 'ph', 'pi', 'pl', 'pla', 'po', 'pol', 'por', 'pos', 'pou', 'pp', 'pr', 'pre', 'pri', 'pro', 'pt', 'pu', 'q', 'qu', 'que', 'qui', 'r', 'r ', 'ra', 'ra ', 'rac', 'ran', 'rat', 'rc', 'rd', 'rd ', 'rde', 're', 're ', 'rea', 'rec', 'rem', 'ren', 'res', 'rg', 'rge', 'ri', 'ri ', 'ric', 'rie', 'rin', 'ris', 'rit', 'rk', 'rko', 'rl', 'rm', 'rn', 'rna', 'ro', 'ro ', 'rom', 'ron', 'ros', 'rr', 'rre', 'rs', 'rs ', 'rt', 'rt ', 'rte', 'rti', 'ru', 'rv', 'ry', 'ry ', 'ré', 's', 's ', 'sa', 'sa ', 'san', 'sar', 'sc', 'sch', 'sco', 'se', 'se ', 'sen', 'ser', 'sh', 'si', 'si ', 'sid', 'sie', 'sin', 'sio', 'sit', 'sl', 'sm', 'so', 'so ', 'son', 'sp', 'spe', 'spo', 'ss', 'ss ', 'sse', 'ssi', 'st', 'st ', 'sta', 'ste', 'sti', 'sto', 'str', 'su', 'su ', 'sur', 't', 't ', 'ta', 'ta ', 'tal', 'tan', 'tar', 'tat', 'te', 'te ', 'ted', 'tel', 'tem', 'ten', 'ter', 'tes', 'th', 'the', 'ti', 'ti ', 'tic', 'tie', 'tin', 'tio', 'to', 'to ', 'ton', 'tor', 'tr', 'tra', 'tre', 'tri', 'tro', 'ts', 'ts ', 'tt', 'tte', 'tu', 'tur', 'tv', 'tw', 'twe', 'twi', 'tz', 'té', 'u', 'u ', 'ua', 'ub', 'uc', 'uch', 'ud', 'ue', 'ue ', 'ues', 'uf', 'ug', 'ui', 'uit', 'ul', 'um', 'um ', 'un', 'un ', 'una', 'und', 'une', 'ung', 'uni', 'unt', 'uo', 'up', 'ur', 'ur ', 'ure', 'us', 'us ', 'ut', 'ut ', 'ute', 'uv', 'uw', 'v', 'v ', 'va', 'van', 've', 've ', 'ven', 'ver', 'vi', 'vid', 'vie', 'vis', 'vo', 'voo', 'vr', 'w', 'w ', 'wa', 'we', 'we ', 'wee', 'wer', 'wi', 'wir', 'wit', 'wo', 'x', 'x ', 'y', 'y ', 'ye', 'yo', 'you', 'yp', 'ypt', 'ys', 'z', 'z ', 'za', 'ze', 'zi', 'zie', 'zio', 'zo', 'zu', 'zy', 'zy ', 'á', 'ä', 'è', 'è ', 'é', 'é ', 'ée', 'és', 'í', 'ñ', 'ó', 'ón', 'ü', '一', '不', '中', '人', '军', '国', '大', '战', '球', '美', '！', '：', '？']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  2,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  2,  0,  1,  0,  1,  2,  2,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  2,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  4,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  2,\n",
       "         1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,\n",
       "         0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  9,  2,  0,  0,\n",
       "         0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,\n",
       "         0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         3,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  2,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         1,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vec.get_feature_names())\n",
    "result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建分类模型\n",
    "#### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909159727479182"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗一下数据，看下效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump images are now more popular than cat gifs\n",
      "我爱你中国China\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_noise(document):\n",
    "    noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\", \"\\d+\", \"，\", \"！\", \"。\", '“', '”', \"？\", \"\\.+\"]))\n",
    "    clean_text = re.sub(noise_pattern, \"\", document)\n",
    "    \n",
    "    return clean_text.strip()\n",
    "\n",
    "\n",
    "print(remove_noise(\"123Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html\"))\n",
    "print(remove_noise(\"我爱你，中国！China520\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我爱你，中国！\n"
     ]
    }
   ],
   "source": [
    "# 清理中文中的英文字母\n",
    "def clean_chinese(document):\n",
    "    pattern = re.compile('[A-Za-z]')\n",
    "    clean_text = re.sub(pattern, \"\", document)\n",
    "    return clean_text.strip()\n",
    "print(clean_chinese(\"我爱你，中国！China\"))\n",
    "data.loc[data['nl'] == 'ch', '1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme'] = data.loc[data['nl'] == 'ch', '1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme'].apply(lambda x: clean_chinese(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 3),\n",
       "        preprocessor=<function remove_noise at 0x1a182a2840>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(\n",
    "    lowercase=True,  # 英文文本全小写\n",
    "    analyzer='char_wb',  # 逐个字母解析\n",
    "    ngram_range=(1, 3),  # 1=出现的字母以及每个字母出现的次数，2=出现的连续2个字母，和连续2个字母出现的次数\n",
    "    # trump images are now... => 1gram=t,r,u,m,p... 2gram=tr,ru,um,mp...\n",
    "    max_features=1000,   # keep the most common 1000 ngrams\n",
    "    preprocessor = remove_noise\n",
    ")\n",
    "vec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier2 = MultinomialNB()\n",
    "classifier2.fit(vec.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9931869795609387"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2.score(vec.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ch       1.00      0.98      0.99       329\n",
      "          de       0.99      1.00      0.99       402\n",
      "          en       0.98      0.99      0.98       365\n",
      "          es       0.99      0.99      0.99       402\n",
      "          fr       0.99      1.00      0.99       390\n",
      "          it       0.99      0.99      0.99       384\n",
      "          nl       0.99      0.99      0.99       370\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2642\n",
      "   macro avg       0.99      0.99      0.99      2642\n",
      "weighted avg       0.99      0.99      0.99      2642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗之前的结果\n",
    "print(classification_report(y_test, classifier.predict(vec.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ch       1.00      1.00      1.00       329\n",
      "          de       0.99      1.00      0.99       402\n",
      "          en       0.99      0.99      0.99       365\n",
      "          es       0.99      0.99      0.99       402\n",
      "          fr       0.99      1.00      0.99       390\n",
      "          it       0.99      0.99      0.99       384\n",
      "          nl       0.99      0.99      0.99       370\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2642\n",
      "   macro avg       0.99      0.99      0.99      2642\n",
      "weighted avg       0.99      0.99      0.99      2642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 数据清洗之后的效果\n",
    "print(classification_report(y_test, classifier2.predict(vec.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
